{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89026e99",
        "outputId": "1790660f-22f9-452a-a11d-be05f1a4f72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=c14c1e7d2a36897fcef17c9e3f5e685b5d1d9f1779b9a01cdfd34f124ba39b59\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets, evaluation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)"
      ],
      "id": "89026e99"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a9e73b6e"
      },
      "outputs": [],
      "source": [
        "# train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
        "\n",
        "# Load the test dataset into a pandas dataframe.\n",
        "import h5py\n",
        "import os\n",
        "\"\"\"\n",
        "eval_df = pd.read_csv('https://raw.githubusercontent.com/sanromra/imdb_data/main/test_IMDB.tsv', delimiter='\\t', header=None)\n",
        "\n",
        "text_col=eval_df.columns.values[0] \n",
        "category_col=eval_df.columns.values[1]\n",
        "\"\"\"\n",
        "test_data =  h5py.File(os.path.join(\"gdrive/MyDrive/DL_Experiments/TextExperiments/data/new_data/dbpedia\", 'processed_test.h5'), 'r')\n",
        "\n",
        "x_eval = torch.tensor(np.array(test_data['text_embeddings']))\n",
        "y_eval = torch.tensor(np.array(test_data['labels']))"
      ],
      "id": "a9e73b6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "84e8f16c128a4ab9905a8e74c749032d"
          ]
        },
        "id": "bac30fda",
        "outputId": "6c331abd-4de0-4dec-de9c-bd1ac5239676",
        "cellView": "form"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84e8f16c128a4ab9905a8e74c749032d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title SetFit\n",
        "# st_model = 'paraphrase-mpnet-base-v2' #@param ['paraphrase-mpnet-base-v2', 'all-mpnet-base-v1', 'all-mpnet-base-v2', 'stsb-mpnet-base-v2', 'all-MiniLM-L12-v2', 'paraphrase-albert-small-v2', 'all-roberta-large-v1']\n",
        "# num_training = 32 #@param [\"8\", \"16\", \"32\", \"54\", \"128\", \"256\", \"512\"] {type:\"raw\"}\n",
        "\n",
        "# set_seed(0)\n",
        "\n",
        "# orig_model = SentenceTransformer(st_model)\n",
        "\n",
        "# X_eval_noFT = orig_model.encode(x_eval, show_progress_bar = True, device = 'mps')"
      ],
      "id": "bac30fda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJvfJ1DEJGL4"
      },
      "source": [
        "#IID"
      ],
      "id": "mJvfJ1DEJGL4"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "521c3930"
      },
      "outputs": [],
      "source": [
        "x_train_total = []\n",
        "x_train_federated = []\n",
        "y_train_total = []\n",
        "y_train_federated = []\n",
        "\n",
        "train_data = h5py.File(os.path.join(\"gdrive/MyDrive/DL_Experiments/TextExperiments/data/new_data/dbpedia\", 'processed_train.h5'), 'r')\n",
        "\n",
        "x_train_total = torch.tensor(np.array(train_data['text_embeddings']))\n",
        "y_train_total = torch.tensor(np.array(train_data['labels']))\n",
        "\n",
        "# net = nn.Sequential(nn.Linear(768, 100),nn.ReLU(),nn.Linear(100, 2)).to('mps')\n",
        "\n",
        "for i in range(14):\n",
        "    exp_results = torch.load('gdrive/MyDrive/DL_Experiments/TextExperiments/Federated_Learning/result/dbpedia_noniidv2_silos/dbpedia_noniidv2_silo'+str(i)+ '.pt', map_location = 'cuda')\n",
        "    x_train_federated.append(exp_results['data'][0][0])\n",
        "    y_train_federated.append(exp_results['data'][0][1])"
      ],
      "id": "521c3930"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "61f81510"
      },
      "outputs": [],
      "source": [
        "x_train_federated = torch.cat(tuple(x_train_federated), dim = 0)\n",
        "y_train_federated = torch.cat(tuple(y_train_federated), dim = 0)"
      ],
      "id": "61f81510"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af0PZcPeJJm6"
      },
      "source": [
        "#Non IID"
      ],
      "id": "af0PZcPeJJm6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g318UWm2JLMl"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import os\n",
        "\n",
        "train_data =  h5py.File(os.path.join(\"gdrive/MyDrive/DL_Experiments/TextExperiments/data/new_data/yahoo\", 'processed_train.h5'), 'r')\n",
        "\n",
        "x_train_total = torch.tensor(np.array(train_data['text_embeddings']))\n",
        "y_train_total = torch.tensor(np.array(train_data['labels']))"
      ],
      "id": "g318UWm2JLMl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsqvkP7LJcIK"
      },
      "outputs": [],
      "source": [
        "exp_results = torch.load('gdrive/MyDrive/DL_Experiments/TextExperiments/Federated_Learning/result/yahoo/whole/res_DM_yahoo_MLP_10ipc.pt', map_location = 'cpu')"
      ],
      "id": "SsqvkP7LJcIK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGLOLX1zJd9f"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "ipc = 10 ## Equal to num silos\n",
        "x_train_split = [[] for i in range(ipc)]\n",
        "y_train_split = [[] for i in range(ipc)]\n",
        "for i in range(num_classes):\n",
        "    x_train_class = x_train_total[y_train_total == i]\n",
        "    y_train_class = y_train_total[y_train_total == i]\n",
        "    centers = exp_results['data'][0][0][exp_results['data'][0][1] == i]\n",
        "    distances = torch.cdist(x_train_class, centers)\n",
        "    closest_centers = torch.argmin(distances, dim=1)\n",
        "    for i in range(ipc):\n",
        "        x_train_split[i].append(x_train_class[closest_centers == i])\n",
        "        y_train_split[i].append(y_train_class[closest_centers == i])"
      ],
      "id": "TGLOLX1zJd9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB-hJWlcJhDI"
      },
      "outputs": [],
      "source": [
        "x_train_federated = [torch.cat(x_train_split[i]) for i in range(ipc)]\n",
        "y_train_federated = [torch.cat(y_train_split[i]) for i in range(ipc)]\n",
        "x_train_federated = torch.cat(tuple(x_train_federated), dim = 0)\n",
        "y_train_federated = torch.cat(tuple(y_train_federated), dim = 0)"
      ],
      "id": "DB-hJWlcJhDI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DC-ZXcWJLpw"
      },
      "source": [
        "#Train"
      ],
      "id": "-DC-ZXcWJLpw"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "14b14239"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "embed_dim = 768\n",
        "hidden_dim = 512\n",
        "num_classes = 14\n",
        "net = nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            #nn.Linear(hidden_dim, hidden_dim),\n",
        "            #nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes))"
      ],
      "id": "14b14239"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2a11fb2a"
      },
      "outputs": [],
      "source": [
        "#X_train_noFT = orig_model.encode(x_train_total, show_progress_bar = True, device = 'cuda')\n",
        "X_eval_noFT = x_eval\n",
        "X_train_noFT = x_train_total"
      ],
      "id": "2a11fb2a"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8a549155"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "# trainset = TensorDataset(torch.Tensor(exp_results1['data'][0][0]),torch.Tensor(exp_results1['data'][0][1])) # create your datset\n",
        "federated_trainset = TensorDataset(torch.Tensor(x_train_federated),torch.Tensor(y_train_federated)) # create your datset\n",
        "federated_trainloader = torch.utils.data.DataLoader(federated_trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "trainset = TensorDataset(torch.Tensor(X_train_noFT),torch.Tensor(y_train_total)) # create your datset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "valset = TensorDataset(torch.Tensor(X_eval_noFT),torch.Tensor(y_eval)) # create your datset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=16, num_workers=0)"
      ],
      "id": "8a549155"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3503042d",
        "outputId": "855df8b4-3e3d-48f0-8445-14a77c877396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1414, -0.2522,  0.0378,  ...,  0.0334, -0.0008, -0.0979],\n",
              "        [-0.0394, -0.0940,  0.0297,  ..., -0.0087,  0.1567, -0.0236],\n",
              "        [ 0.1579, -0.1827, -0.0556,  ..., -0.1233, -0.0069, -0.0621],\n",
              "        ...,\n",
              "        [-0.1128, -0.0594,  0.0581,  ...,  0.0494, -0.0097, -0.1067],\n",
              "        [-0.0240, -0.0873,  0.1267,  ..., -0.0947,  0.1797,  0.1440],\n",
              "        [ 0.1409,  0.1106,  0.0533,  ..., -0.0623, -0.1377, -0.0983]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_eval_noFT"
      ],
      "id": "3503042d"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e65b7f0",
        "outputId": "f8d1ac51-e7d0-4965-8167-6faae085e879"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(455000), 70000, 70000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sum(y_eval), len(y_eval), len(X_eval_noFT)"
      ],
      "id": "3e65b7f0"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "55d5eee8"
      },
      "outputs": [],
      "source": [
        "Epoch = 900\n",
        "lr_schedule = []#[Epoch//2+1]\n",
        "lr = 0.01\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)#, weight_decay=0.0005)"
      ],
      "id": "55d5eee8"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a5a69ded"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "id": "a5a69ded"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a0beb7a2"
      },
      "outputs": [],
      "source": [
        "def epoch(mode, dataloader, net, optimizer, criterion):\n",
        "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
        "    net = net.to(device)\n",
        "    #print(\"Net: {}\".format(net.is_cuda))\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    if mode == 'train':\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    for i_batch, datum in enumerate(dataloader):\n",
        "        img = datum[0].float().to(device)\n",
        "#         print(\"Sample: {}\".format(img.is_cuda))\n",
        "        lab = datum[1].long().to(device)\n",
        "#         print(\"Label: {}\".format(lab.is_cuda))\n",
        "        n_b = lab.shape[0]\n",
        "\n",
        "        output = net(img).to(device)\n",
        "        loss = criterion(output, lab)\n",
        "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
        "\n",
        "        loss_avg += loss.item()*n_b\n",
        "        acc_avg += acc\n",
        "        num_exp += n_b\n",
        "\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    loss_avg /= num_exp\n",
        "    acc_avg /= num_exp\n",
        "\n",
        "    return loss_avg, acc_avg"
      ],
      "id": "a0beb7a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c7b28b"
      },
      "source": [
        "### Training whole dataset "
      ],
      "id": "f8c7b28b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1302dc5d",
        "outputId": "21d1e684-f4ba-46b9-fe72-2899a8763cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7024\n",
            "0.70505\n",
            "0.7069166666666666\n",
            "0.7074\n",
            "0.7086333333333333\n",
            "0.7094666666666667\n",
            "0.7096666666666667\n",
            "0.7103333333333334\n",
            "0.71155\n",
            "0.7105833333333333\n",
            "0.7114833333333334\n",
            "0.7121666666666666\n",
            "0.7118666666666666\n",
            "0.7127666666666667\n",
            "0.7124333333333334\n",
            "0.7127666666666667\n",
            "0.7125666666666667\n",
            "0.71245\n",
            "0.71385\n",
            "0.7131166666666666\n",
            "0.7128666666666666\n",
            "0.7135833333333333\n",
            "0.71315\n",
            "0.7137833333333333\n",
            "0.7137833333333333\n",
            "0.71445\n",
            "0.7133833333333334\n",
            "0.7144666666666667\n",
            "0.7138666666666666\n",
            "0.7125333333333334\n",
            "0.7142833333333334\n",
            "0.7144333333333334\n",
            "0.7142333333333334\n",
            "0.7149333333333333\n",
            "0.7139\n",
            "0.7144333333333334\n",
            "0.7137333333333333\n",
            "0.7143333333333334\n",
            "0.7139\n",
            "0.7142666666666667\n",
            "0.71475\n",
            "0.7146666666666667\n",
            "0.7149166666666666\n",
            "0.7144\n",
            "0.7146666666666667\n",
            "0.7136666666666667\n",
            "0.7137666666666667\n",
            "0.7147\n",
            "0.7139166666666666\n",
            "0.71455\n",
            "0.7148333333333333\n",
            "0.7146666666666667\n",
            "0.7149666666666666\n",
            "0.7147\n",
            "0.7140833333333333\n",
            "0.7142333333333334\n",
            "0.7144333333333334\n",
            "0.71455\n",
            "0.7148833333333333\n",
            "0.7140333333333333\n",
            "0.7138833333333333\n",
            "0.7153\n",
            "0.71395\n",
            "0.7155666666666667\n",
            "0.7143666666666667\n",
            "0.7142833333333334\n",
            "0.71525\n",
            "0.7149166666666666\n",
            "0.71505\n",
            "0.71515\n",
            "0.7144666666666667\n",
            "0.7148333333333333\n",
            "0.7146666666666667\n",
            "0.71415\n",
            "0.7145166666666667\n",
            "0.7140833333333333\n",
            "0.7146666666666667\n",
            "0.71395\n",
            "0.71515\n",
            "0.7152333333333334\n",
            "0.715\n",
            "0.7142166666666667\n",
            "0.7146166666666667\n",
            "0.7149833333333333\n",
            "0.7137333333333333\n",
            "0.7151166666666666\n",
            "0.71455\n",
            "0.71575\n",
            "0.7152166666666666\n",
            "0.7146\n",
            "0.71445\n",
            "0.7146833333333333\n",
            "0.7154666666666667\n",
            "0.7147833333333333\n",
            "0.7148833333333333\n",
            "0.7142166666666667\n",
            "0.7147333333333333\n",
            "0.7157666666666667\n",
            "0.7152666666666667\n",
            "0.7139\n",
            "0.71415\n",
            "0.7144666666666667\n",
            "0.7159333333333333\n",
            "0.7143\n",
            "0.7153333333333334\n",
            "0.71445\n",
            "0.7147333333333333\n",
            "0.7148833333333333\n",
            "0.7151166666666666\n",
            "0.7146833333333333\n",
            "0.71435\n",
            "0.7146833333333333\n",
            "0.7138666666666666\n",
            "0.7144\n",
            "0.71405\n",
            "0.7150333333333333\n",
            "0.7137166666666667\n",
            "0.71465\n",
            "0.71465\n",
            "0.7146\n",
            "0.71475\n",
            "0.7150333333333333\n",
            "0.7150166666666666\n",
            "0.7155\n",
            "0.7150666666666666\n",
            "0.7143\n",
            "0.7139666666666666\n",
            "0.7149333333333333\n",
            "0.7144666666666667\n",
            "0.7150333333333333\n",
            "0.7146\n",
            "0.7149833333333333\n",
            "0.7146666666666667\n",
            "0.71465\n",
            "0.7134166666666667\n",
            "0.7148\n",
            "0.71525\n",
            "0.71355\n",
            "0.7148333333333333\n",
            "0.7152333333333334\n",
            "0.7157166666666667\n",
            "0.7149833333333333\n",
            "0.7152666666666667\n",
            "0.7146333333333333\n",
            "0.7147333333333333\n",
            "0.7145166666666667\n",
            "0.7146333333333333\n",
            "0.71525\n",
            "0.7149\n",
            "0.7143\n",
            "0.71505\n",
            "0.71505\n",
            "0.7154666666666667\n",
            "0.7150833333333333\n",
            "0.7156\n",
            "0.7152833333333334\n",
            "0.7149666666666666\n",
            "0.71565\n",
            "0.7155333333333334\n",
            "0.7155166666666667\n",
            "0.71545\n",
            "0.7153833333333334\n",
            "0.71525\n",
            "0.7154666666666667\n",
            "0.7151333333333333\n",
            "0.7156833333333333\n",
            "0.7151333333333333\n",
            "0.7153\n",
            "0.71525\n",
            "0.71505\n",
            "0.71535\n",
            "0.7152\n",
            "0.71545\n",
            "0.7155\n",
            "0.7150833333333333\n",
            "0.7153833333333334\n",
            "0.7158666666666667\n",
            "0.71555\n",
            "0.7154166666666667\n",
            "0.71585\n",
            "0.7153833333333334\n",
            "0.71545\n",
            "0.7157833333333333\n",
            "0.7153333333333334\n",
            "0.7152666666666667\n",
            "0.7155166666666667\n",
            "0.71525\n",
            "0.7154\n",
            "0.71505\n",
            "0.71585\n",
            "0.7157\n",
            "0.7153333333333334\n",
            "0.7152833333333334\n",
            "0.7157\n",
            "0.7154333333333334\n",
            "0.7157333333333333\n",
            "0.7158333333333333\n",
            "0.7155666666666667\n",
            "0.7155666666666667\n",
            "0.71535\n",
            "0.7154166666666667\n",
            "0.71555\n",
            "0.7150833333333333\n",
            "0.7154166666666667\n",
            "0.71575\n",
            "0.7155333333333334\n",
            "0.7155166666666667\n",
            "0.7155666666666667\n",
            "0.7157333333333333\n",
            "0.71555\n",
            "0.7154\n",
            "0.7155333333333334\n",
            "0.7154333333333334\n",
            "0.7153666666666667\n",
            "0.7149666666666666\n",
            "0.7158666666666667\n",
            "0.7153333333333334\n",
            "0.71545\n",
            "0.71575\n",
            "0.7157333333333333\n",
            "0.71535\n",
            "0.7155666666666667\n",
            "0.7152\n",
            "0.7154166666666667\n",
            "0.7156666666666667\n",
            "0.7155333333333334\n",
            "0.71585\n",
            "0.7155666666666667\n",
            "0.7158\n",
            "0.7158833333333333\n",
            "0.7157333333333333\n",
            "0.7156666666666667\n",
            "0.7154333333333334\n",
            "0.7154333333333334\n",
            "0.7155833333333333\n",
            "0.7156\n",
            "0.7154166666666667\n",
            "0.7155166666666667\n",
            "0.7154833333333334\n",
            "0.7152333333333334\n",
            "0.7156166666666667\n",
            "0.7157833333333333\n",
            "0.71565\n",
            "0.7156666666666667\n",
            "0.7156\n",
            "0.7153\n",
            "0.7155\n",
            "0.7152666666666667\n",
            "0.7153666666666667\n",
            "0.7151333333333333\n",
            "0.7155166666666667\n",
            "0.7157833333333333\n",
            "0.7157833333333333\n",
            "0.7154833333333334\n",
            "0.7160166666666666\n",
            "0.7153166666666667\n",
            "0.7156166666666667\n",
            "0.7155666666666667\n",
            "0.7151\n",
            "0.7154833333333334\n",
            "0.7156833333333333\n",
            "0.7156166666666667\n",
            "0.71565\n",
            "0.7156333333333333\n",
            "0.7156666666666667\n",
            "0.7156166666666667\n",
            "0.7155\n",
            "0.71565\n",
            "0.71575\n",
            "0.7157166666666667\n",
            "0.7152833333333334\n",
            "0.7155333333333334\n",
            "0.7157333333333333\n",
            "0.71585\n",
            "0.71605\n",
            "0.7154666666666667\n",
            "0.71565\n",
            "0.7155333333333334\n",
            "0.7153333333333334\n",
            "0.7150666666666666\n",
            "0.7158333333333333\n",
            "0.7150666666666666\n",
            "0.7157833333333333\n",
            "0.7155166666666667\n",
            "0.7154166666666667\n",
            "0.7156666666666667\n",
            "0.71555\n",
            "0.7156833333333333\n",
            "0.7156833333333333\n",
            "0.7153\n",
            "0.7154666666666667\n",
            "0.7156\n",
            "0.7153\n",
            "0.7155833333333333\n",
            "0.7151333333333333\n",
            "0.7154833333333334\n",
            "0.7155833333333333\n",
            "0.7152833333333334\n",
            "0.7155833333333333\n",
            "0.7155333333333334\n",
            "0.7158166666666667\n"
          ]
        }
      ],
      "source": [
        "for ep in range(Epoch+1):\n",
        "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion)\n",
        "        if ep in lr_schedule:\n",
        "            lr *= 0.1\n",
        "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "        loss, acc = epoch('test', valloader, net, optimizer, criterion)\n",
        "        print(acc)"
      ],
      "id": "1302dc5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d290121",
        "outputId": "e7610ce5-c054-4292-913b-a0dc22187eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8586611078302065, 0.7216166666666667)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epoch('test', valloader, net, optimizer, criterion)"
      ],
      "id": "7d290121"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a53888f"
      },
      "source": [
        "### Training 10 Silos "
      ],
      "id": "0a53888f"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c656d748"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            #nn.Linear(hidden_dim, hidden_dim),\n",
        "            #nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes))\n",
        "lr = 0.01\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)#, weight_decay=0.0005)"
      ],
      "id": "c656d748"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7ae2880",
        "outputId": "8c718627-13a6-47ae-97ee-c9fdcab7e25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 0.0818\n",
            "Epoch 1: 0.08934285714285714\n",
            "Epoch 2: 0.1013\n",
            "Epoch 3: 0.10708571428571428\n",
            "Epoch 4: 0.09412857142857142\n",
            "Epoch 5: 0.08181428571428571\n",
            "Epoch 6: 0.07404285714285715\n",
            "Epoch 7: 0.07157142857142858\n",
            "Epoch 8: 0.07142857142857142\n",
            "Epoch 9: 0.07142857142857142\n",
            "Epoch 10: 0.07142857142857142\n",
            "Epoch 11: 0.07142857142857142\n",
            "Epoch 12: 0.07142857142857142\n",
            "Epoch 13: 0.07142857142857142\n",
            "Epoch 14: 0.07142857142857142\n",
            "Epoch 15: 0.07142857142857142\n",
            "Epoch 16: 0.07142857142857142\n",
            "Epoch 17: 0.07142857142857142\n",
            "Epoch 18: 0.07142857142857142\n",
            "Epoch 19: 0.07142857142857142\n",
            "Epoch 20: 0.07142857142857142\n",
            "Epoch 21: 0.07142857142857142\n",
            "Epoch 22: 0.07142857142857142\n",
            "Epoch 23: 0.07142857142857142\n",
            "Epoch 24: 0.07142857142857142\n",
            "Epoch 25: 0.07142857142857142\n",
            "Epoch 26: 0.07142857142857142\n",
            "Epoch 27: 0.07142857142857142\n",
            "Epoch 28: 0.07142857142857142\n",
            "Epoch 29: 0.07142857142857142\n",
            "Epoch 30: 0.07142857142857142\n",
            "Epoch 31: 0.07142857142857142\n",
            "Epoch 32: 0.07142857142857142\n",
            "Epoch 33: 0.07142857142857142\n",
            "Epoch 34: 0.07142857142857142\n",
            "Epoch 35: 0.07142857142857142\n",
            "Epoch 36: 0.07142857142857142\n",
            "Epoch 37: 0.07142857142857142\n",
            "Epoch 38: 0.07142857142857142\n",
            "Epoch 39: 0.07142857142857142\n",
            "Epoch 40: 0.07142857142857142\n",
            "Epoch 41: 0.07142857142857142\n",
            "Epoch 42: 0.07142857142857142\n",
            "Epoch 43: 0.07142857142857142\n",
            "Epoch 44: 0.07142857142857142\n",
            "Epoch 45: 0.07145714285714286\n",
            "Epoch 46: 0.07211428571428571\n",
            "Epoch 47: 0.0757\n",
            "Epoch 48: 0.08945714285714286\n",
            "Epoch 49: 0.12032857142857142\n",
            "Epoch 50: 0.15598571428571428\n",
            "Epoch 51: 0.18354285714285715\n",
            "Epoch 52: 0.20582857142857142\n",
            "Epoch 53: 0.22564285714285715\n",
            "Epoch 54: 0.24652857142857143\n",
            "Epoch 55: 0.26135714285714284\n",
            "Epoch 56: 0.2707\n",
            "Epoch 57: 0.2809285714285714\n",
            "Epoch 58: 0.2893857142857143\n",
            "Epoch 59: 0.29895714285714287\n",
            "Epoch 60: 0.3072714285714286\n",
            "Epoch 61: 0.3120857142857143\n",
            "Epoch 62: 0.314\n",
            "Epoch 63: 0.3166285714285714\n",
            "Epoch 64: 0.3273\n",
            "Epoch 65: 0.3422142857142857\n",
            "Epoch 66: 0.3551142857142857\n",
            "Epoch 67: 0.3647142857142857\n",
            "Epoch 68: 0.37417142857142854\n",
            "Epoch 69: 0.3805857142857143\n",
            "Epoch 70: 0.3822\n",
            "Epoch 71: 0.38404285714285713\n",
            "Epoch 72: 0.3871857142857143\n",
            "Epoch 73: 0.39371428571428574\n",
            "Epoch 74: 0.40777142857142856\n",
            "Epoch 75: 0.4247714285714286\n",
            "Epoch 76: 0.43788571428571427\n",
            "Epoch 77: 0.4476428571428571\n",
            "Epoch 78: 0.4553285714285714\n",
            "Epoch 79: 0.4614857142857143\n",
            "Epoch 80: 0.46855714285714284\n",
            "Epoch 81: 0.4749\n",
            "Epoch 82: 0.4809857142857143\n",
            "Epoch 83: 0.4861285714285714\n",
            "Epoch 84: 0.4897\n",
            "Epoch 85: 0.49444285714285713\n",
            "Epoch 86: 0.4997714285714286\n",
            "Epoch 87: 0.5041714285714286\n",
            "Epoch 88: 0.5072571428571429\n",
            "Epoch 89: 0.5096428571428572\n",
            "Epoch 90: 0.5120571428571429\n",
            "Epoch 91: 0.5167\n",
            "Epoch 92: 0.5223\n",
            "Epoch 93: 0.5282285714285714\n",
            "Epoch 94: 0.5329285714285714\n",
            "Epoch 95: 0.5363285714285714\n",
            "Epoch 96: 0.5384285714285715\n",
            "Epoch 97: 0.5393285714285714\n",
            "Epoch 98: 0.5393142857142857\n",
            "Epoch 99: 0.5382428571428571\n",
            "Epoch 100: 0.5368857142857143\n",
            "Epoch 101: 0.5354857142857142\n",
            "Epoch 102: 0.5348428571428572\n",
            "Epoch 103: 0.5330857142857143\n",
            "Epoch 104: 0.5314285714285715\n",
            "Epoch 105: 0.5292285714285714\n",
            "Epoch 106: 0.5293571428571429\n",
            "Epoch 107: 0.5336428571428572\n",
            "Epoch 108: 0.5379142857142857\n",
            "Epoch 109: 0.5432285714285714\n",
            "Epoch 110: 0.5494857142857142\n",
            "Epoch 111: 0.5530571428571428\n",
            "Epoch 112: 0.5534142857142857\n",
            "Epoch 113: 0.5545857142857142\n",
            "Epoch 114: 0.5577142857142857\n",
            "Epoch 115: 0.5617\n",
            "Epoch 116: 0.5675142857142857\n",
            "Epoch 117: 0.5729\n",
            "Epoch 118: 0.5763571428571429\n",
            "Epoch 119: 0.5782285714285714\n",
            "Epoch 120: 0.5802\n",
            "Epoch 121: 0.5819571428571428\n",
            "Epoch 122: 0.5831285714285714\n",
            "Epoch 123: 0.5868142857142857\n",
            "Epoch 124: 0.5914714285714285\n",
            "Epoch 125: 0.5948142857142857\n",
            "Epoch 126: 0.5966285714285714\n",
            "Epoch 127: 0.5966428571428571\n",
            "Epoch 128: 0.5972285714285714\n",
            "Epoch 129: 0.5958714285714286\n",
            "Epoch 130: 0.5926857142857143\n",
            "Epoch 131: 0.5913142857142857\n",
            "Epoch 132: 0.5914142857142857\n",
            "Epoch 133: 0.5930285714285715\n",
            "Epoch 134: 0.5942571428571428\n",
            "Epoch 135: 0.5963285714285714\n",
            "Epoch 136: 0.6030714285714286\n",
            "Epoch 137: 0.6093285714285714\n",
            "Epoch 138: 0.6144857142857143\n",
            "Epoch 139: 0.6216857142857143\n",
            "Epoch 140: 0.6296428571428572\n",
            "Epoch 141: 0.6323\n",
            "Epoch 142: 0.6316428571428572\n",
            "Epoch 143: 0.6293142857142857\n",
            "Epoch 144: 0.6251571428571429\n",
            "Epoch 145: 0.622\n",
            "Epoch 146: 0.6198428571428571\n",
            "Epoch 147: 0.6175571428571428\n",
            "Epoch 148: 0.6146\n",
            "Epoch 149: 0.6167142857142857\n",
            "Epoch 150: 0.6239428571428571\n",
            "Epoch 151: 0.6311142857142857\n",
            "Epoch 152: 0.6416428571428572\n",
            "Epoch 153: 0.6537714285714286\n",
            "Epoch 154: 0.6686285714285715\n",
            "Epoch 155: 0.6847\n",
            "Epoch 156: 0.6971\n",
            "Epoch 157: 0.7091\n",
            "Epoch 158: 0.7206142857142858\n",
            "Epoch 159: 0.7315857142857143\n",
            "Epoch 160: 0.7432142857142857\n",
            "Epoch 161: 0.7544\n",
            "Epoch 162: 0.7625\n",
            "Epoch 163: 0.7666714285714286\n",
            "Epoch 164: 0.7663428571428571\n",
            "Epoch 165: 0.7625857142857143\n",
            "Epoch 166: 0.7590714285714286\n",
            "Epoch 167: 0.7544285714285714\n",
            "Epoch 168: 0.7482714285714286\n",
            "Epoch 169: 0.7440142857142857\n",
            "Epoch 170: 0.7396285714285714\n",
            "Epoch 171: 0.7323285714285714\n",
            "Epoch 172: 0.7225857142857143\n",
            "Epoch 173: 0.7054\n",
            "Epoch 174: 0.6877\n",
            "Epoch 175: 0.6812857142857143\n",
            "Epoch 176: 0.6799857142857143\n",
            "Epoch 177: 0.6836428571428571\n",
            "Epoch 178: 0.6903142857142858\n",
            "Epoch 179: 0.6989571428571428\n",
            "Epoch 180: 0.7091428571428572\n",
            "Epoch 181: 0.7149\n",
            "Epoch 182: 0.7214714285714285\n",
            "Epoch 183: 0.7328142857142858\n",
            "Epoch 184: 0.7459857142857143\n",
            "Epoch 185: 0.7615428571428572\n",
            "Epoch 186: 0.7806\n",
            "Epoch 187: 0.7997\n",
            "Epoch 188: 0.8142142857142857\n",
            "Epoch 189: 0.8225571428571429\n",
            "Epoch 190: 0.8278285714285715\n",
            "Epoch 191: 0.8299857142857143\n",
            "Epoch 192: 0.8273714285714285\n",
            "Epoch 193: 0.8192285714285714\n",
            "Epoch 194: 0.8094285714285714\n",
            "Epoch 195: 0.8011142857142857\n",
            "Epoch 196: 0.7955428571428571\n",
            "Epoch 197: 0.7911857142857143\n",
            "Epoch 198: 0.7904714285714286\n",
            "Epoch 199: 0.7939142857142857\n",
            "Epoch 200: 0.7993285714285714\n",
            "Epoch 201: 0.8046714285714286\n",
            "Epoch 202: 0.8109857142857143\n",
            "Epoch 203: 0.8196571428571429\n",
            "Epoch 204: 0.8274428571428571\n",
            "Epoch 205: 0.8349714285714286\n",
            "Epoch 206: 0.8417428571428571\n",
            "Epoch 207: 0.8466\n",
            "Epoch 208: 0.8481714285714286\n",
            "Epoch 209: 0.8490428571428571\n",
            "Epoch 210: 0.8496285714285714\n",
            "Epoch 211: 0.8487857142857143\n",
            "Epoch 212: 0.8457\n",
            "Epoch 213: 0.8366714285714286\n",
            "Epoch 214: 0.8255285714285714\n",
            "Epoch 215: 0.8222285714285714\n",
            "Epoch 216: 0.8243714285714285\n",
            "Epoch 217: 0.8268857142857143\n",
            "Epoch 218: 0.8302\n",
            "Epoch 219: 0.8365857142857143\n",
            "Epoch 220: 0.8462428571428572\n",
            "Epoch 221: 0.8569571428571429\n",
            "Epoch 222: 0.8673428571428572\n",
            "Epoch 223: 0.8781142857142857\n",
            "Epoch 224: 0.8872571428571429\n",
            "Epoch 225: 0.8919714285714285\n",
            "Epoch 226: 0.8937285714285714\n",
            "Epoch 227: 0.8943714285714286\n",
            "Epoch 228: 0.8941571428571429\n",
            "Epoch 229: 0.8896142857142857\n",
            "Epoch 230: 0.8816428571428572\n",
            "Epoch 231: 0.8785285714285714\n",
            "Epoch 232: 0.8779285714285714\n",
            "Epoch 233: 0.8751571428571429\n",
            "Epoch 234: 0.8698571428571429\n",
            "Epoch 235: 0.8672857142857143\n",
            "Epoch 236: 0.8684428571428572\n",
            "Epoch 237: 0.8732428571428571\n",
            "Epoch 238: 0.8812428571428571\n",
            "Epoch 239: 0.8884142857142857\n",
            "Epoch 240: 0.8915285714285714\n",
            "Epoch 241: 0.8920571428571429\n",
            "Epoch 242: 0.8915428571428572\n",
            "Epoch 243: 0.8906714285714286\n",
            "Epoch 244: 0.8913428571428571\n",
            "Epoch 245: 0.8948\n",
            "Epoch 246: 0.8968571428571429\n",
            "Epoch 247: 0.8978571428571429\n",
            "Epoch 248: 0.8951857142857143\n",
            "Epoch 249: 0.8904142857142857\n",
            "Epoch 250: 0.8874714285714286\n",
            "Epoch 251: 0.8840571428571429\n",
            "Epoch 252: 0.8817\n",
            "Epoch 253: 0.8809\n",
            "Epoch 254: 0.8795571428571428\n",
            "Epoch 255: 0.8785571428571428\n",
            "Epoch 256: 0.8782714285714286\n",
            "Epoch 257: 0.8783857142857143\n",
            "Epoch 258: 0.8792428571428571\n",
            "Epoch 259: 0.8812571428571429\n",
            "Epoch 260: 0.8819571428571429\n",
            "Epoch 261: 0.8812857142857143\n",
            "Epoch 262: 0.8807142857142857\n",
            "Epoch 263: 0.8795571428571428\n",
            "Epoch 264: 0.8805714285714286\n",
            "Epoch 265: 0.8862857142857142\n",
            "Epoch 266: 0.8940857142857143\n",
            "Epoch 267: 0.9040714285714285\n",
            "Epoch 268: 0.9133857142857142\n",
            "Epoch 269: 0.9190285714285714\n",
            "Epoch 270: 0.9211285714285714\n",
            "Epoch 271: 0.9174\n",
            "Epoch 272: 0.9111142857142858\n",
            "Epoch 273: 0.9052142857142857\n",
            "Epoch 274: 0.9021428571428571\n",
            "Epoch 275: 0.9018428571428572\n",
            "Epoch 276: 0.9059285714285714\n",
            "Epoch 277: 0.9113571428571429\n",
            "Epoch 278: 0.9141142857142858\n",
            "Epoch 279: 0.9152428571428571\n",
            "Epoch 280: 0.9147714285714286\n",
            "Epoch 281: 0.9132285714285714\n",
            "Epoch 282: 0.9125571428571428\n",
            "Epoch 283: 0.9126142857142857\n",
            "Epoch 284: 0.9139857142857143\n",
            "Epoch 285: 0.9150857142857143\n",
            "Epoch 286: 0.9165142857142857\n",
            "Epoch 287: 0.9189571428571428\n",
            "Epoch 288: 0.9217714285714286\n",
            "Epoch 289: 0.9248428571428572\n",
            "Epoch 290: 0.9265142857142857\n",
            "Epoch 291: 0.9270428571428572\n",
            "Epoch 292: 0.9256571428571428\n",
            "Epoch 293: 0.9245142857142857\n",
            "Epoch 294: 0.9229285714285714\n",
            "Epoch 295: 0.9219714285714286\n",
            "Epoch 296: 0.9199571428571428\n",
            "Epoch 297: 0.9189857142857143\n",
            "Epoch 298: 0.9177571428571428\n",
            "Epoch 299: 0.9159285714285714\n",
            "Epoch 300: 0.9140714285714285\n",
            "Epoch 301: 0.9161571428571429\n",
            "Epoch 302: 0.9198428571428572\n",
            "Epoch 303: 0.9236\n",
            "Epoch 304: 0.9246571428571428\n",
            "Epoch 305: 0.9258714285714286\n",
            "Epoch 306: 0.9258142857142857\n",
            "Epoch 307: 0.9243428571428571\n",
            "Epoch 308: 0.9238428571428572\n",
            "Epoch 309: 0.9227571428571428\n",
            "Epoch 310: 0.9205285714285715\n",
            "Epoch 311: 0.9171571428571429\n",
            "Epoch 312: 0.9160714285714285\n",
            "Epoch 313: 0.9182857142857143\n",
            "Epoch 314: 0.9213428571428571\n",
            "Epoch 315: 0.9238142857142857\n",
            "Epoch 316: 0.9260857142857143\n",
            "Epoch 317: 0.9278142857142857\n",
            "Epoch 318: 0.9299285714285714\n",
            "Epoch 319: 0.9314285714285714\n",
            "Epoch 320: 0.9322142857142857\n",
            "Epoch 321: 0.9321571428571429\n",
            "Epoch 322: 0.9319857142857143\n",
            "Epoch 323: 0.9326428571428571\n",
            "Epoch 324: 0.9331285714285714\n",
            "Epoch 325: 0.9336857142857143\n",
            "Epoch 326: 0.9344285714285714\n",
            "Epoch 327: 0.9351428571428572\n",
            "Epoch 328: 0.9357285714285715\n",
            "Epoch 329: 0.9363142857142858\n",
            "Epoch 330: 0.936\n",
            "Epoch 331: 0.9357428571428571\n",
            "Epoch 332: 0.9356714285714286\n",
            "Epoch 333: 0.9356428571428571\n",
            "Epoch 334: 0.9353285714285714\n",
            "Epoch 335: 0.9347428571428571\n",
            "Epoch 336: 0.9339857142857143\n",
            "Epoch 337: 0.9337714285714286\n",
            "Epoch 338: 0.9339571428571428\n",
            "Epoch 339: 0.9338\n",
            "Epoch 340: 0.9333\n",
            "Epoch 341: 0.9332428571428572\n",
            "Epoch 342: 0.9325285714285714\n",
            "Epoch 343: 0.9313571428571429\n",
            "Epoch 344: 0.9307714285714286\n",
            "Epoch 345: 0.9306142857142857\n",
            "Epoch 346: 0.9306714285714286\n",
            "Epoch 347: 0.9303142857142858\n",
            "Epoch 348: 0.9302\n",
            "Epoch 349: 0.9299\n",
            "Epoch 350: 0.9298428571428572\n",
            "Epoch 351: 0.9306857142857143\n",
            "Epoch 352: 0.9322142857142857\n",
            "Epoch 353: 0.9335428571428571\n",
            "Epoch 354: 0.9343857142857143\n",
            "Epoch 355: 0.9349142857142857\n",
            "Epoch 356: 0.9355714285714286\n",
            "Epoch 357: 0.9367142857142857\n",
            "Epoch 358: 0.9381714285714285\n",
            "Epoch 359: 0.9393714285714285\n",
            "Epoch 360: 0.9407\n",
            "Epoch 361: 0.9412\n",
            "Epoch 362: 0.942\n",
            "Epoch 363: 0.9425571428571429\n",
            "Epoch 364: 0.9426857142857142\n",
            "Epoch 365: 0.9429142857142857\n",
            "Epoch 366: 0.9433285714285714\n",
            "Epoch 367: 0.9428428571428571\n",
            "Epoch 368: 0.9418714285714286\n",
            "Epoch 369: 0.9409\n",
            "Epoch 370: 0.9399142857142857\n",
            "Epoch 371: 0.9389714285714286\n",
            "Epoch 372: 0.9383428571428571\n",
            "Epoch 373: 0.9379428571428572\n",
            "Epoch 374: 0.9385857142857142\n",
            "Epoch 375: 0.9400285714285714\n",
            "Epoch 376: 0.9411142857142857\n",
            "Epoch 377: 0.9415571428571429\n",
            "Epoch 378: 0.9413571428571429\n",
            "Epoch 379: 0.9406714285714286\n",
            "Epoch 380: 0.9396428571428571\n",
            "Epoch 381: 0.9387571428571428\n",
            "Epoch 382: 0.9378571428571428\n",
            "Epoch 383: 0.9369142857142857\n",
            "Epoch 384: 0.9359\n",
            "Epoch 385: 0.9352571428571429\n",
            "Epoch 386: 0.9348857142857143\n",
            "Epoch 387: 0.9347857142857143\n",
            "Epoch 388: 0.9343571428571429\n",
            "Epoch 389: 0.9337571428571428\n",
            "Epoch 390: 0.9336571428571429\n",
            "Epoch 391: 0.9337\n",
            "Epoch 392: 0.9336142857142857\n",
            "Epoch 393: 0.9333714285714285\n",
            "Epoch 394: 0.9334142857142858\n",
            "Epoch 395: 0.9339571428571428\n",
            "Epoch 396: 0.9344428571428571\n",
            "Epoch 397: 0.9343285714285714\n",
            "Epoch 398: 0.9348571428571428\n",
            "Epoch 399: 0.9361857142857143\n",
            "Epoch 400: 0.9371714285714285\n",
            "Epoch 401: 0.9378428571428571\n",
            "Epoch 402: 0.9382142857142857\n",
            "Epoch 403: 0.9382285714285714\n",
            "Epoch 404: 0.9378857142857143\n",
            "Epoch 405: 0.9376428571428571\n",
            "Epoch 406: 0.9372285714285714\n",
            "Epoch 407: 0.9370142857142857\n",
            "Epoch 408: 0.9371428571428572\n",
            "Epoch 409: 0.9373428571428571\n",
            "Epoch 410: 0.9374714285714286\n",
            "Epoch 411: 0.938\n",
            "Epoch 412: 0.9381714285714285\n",
            "Epoch 413: 0.9382285714285714\n",
            "Epoch 414: 0.9384\n",
            "Epoch 415: 0.9383571428571429\n",
            "Epoch 416: 0.9379285714285714\n",
            "Epoch 417: 0.9371571428571429\n",
            "Epoch 418: 0.9363714285714285\n",
            "Epoch 419: 0.9358\n",
            "Epoch 420: 0.9354857142857143\n",
            "Epoch 421: 0.9352714285714285\n",
            "Epoch 422: 0.9358857142857143\n",
            "Epoch 423: 0.9366857142857142\n",
            "Epoch 424: 0.9369571428571428\n",
            "Epoch 425: 0.9370428571428572\n",
            "Epoch 426: 0.9373142857142858\n",
            "Epoch 427: 0.9380857142857143\n",
            "Epoch 428: 0.9388571428571428\n",
            "Epoch 429: 0.9395857142857142\n",
            "Epoch 430: 0.9397428571428571\n",
            "Epoch 431: 0.9398714285714286\n",
            "Epoch 432: 0.9400285714285714\n",
            "Epoch 433: 0.9397\n",
            "Epoch 434: 0.9393714285714285\n",
            "Epoch 435: 0.9389285714285714\n",
            "Epoch 436: 0.9384571428571429\n",
            "Epoch 437: 0.9379142857142857\n",
            "Epoch 438: 0.9372714285714285\n",
            "Epoch 439: 0.9371\n",
            "Epoch 440: 0.9376571428571429\n",
            "Epoch 441: 0.9387142857142857\n",
            "Epoch 442: 0.9392428571428572\n",
            "Epoch 443: 0.9394571428571429\n",
            "Epoch 444: 0.9392428571428572\n",
            "Epoch 445: 0.9397\n",
            "Epoch 446: 0.9396285714285715\n",
            "Epoch 447: 0.9394857142857143\n",
            "Epoch 448: 0.9391857142857143\n",
            "Epoch 449: 0.9390285714285714\n",
            "Epoch 450: 0.9386714285714286\n",
            "Epoch 451: 0.9381\n",
            "Epoch 452: 0.9379\n",
            "Epoch 453: 0.9376428571428571\n",
            "Epoch 454: 0.9374857142857143\n",
            "Epoch 455: 0.9375857142857142\n",
            "Epoch 456: 0.9376285714285715\n",
            "Epoch 457: 0.9380714285714286\n",
            "Epoch 458: 0.9382714285714285\n",
            "Epoch 459: 0.9386428571428571\n",
            "Epoch 460: 0.9387\n",
            "Epoch 461: 0.9388142857142857\n",
            "Epoch 462: 0.9388857142857143\n",
            "Epoch 463: 0.9390428571428572\n",
            "Epoch 464: 0.9390714285714286\n",
            "Epoch 465: 0.9391142857142857\n",
            "Epoch 466: 0.9393428571428571\n",
            "Epoch 467: 0.9396857142857142\n",
            "Epoch 468: 0.9399857142857143\n",
            "Epoch 469: 0.9401714285714285\n",
            "Epoch 470: 0.9404857142857143\n",
            "Epoch 471: 0.9409571428571428\n",
            "Epoch 472: 0.9414142857142858\n",
            "Epoch 473: 0.9418428571428571\n",
            "Epoch 474: 0.9422428571428572\n",
            "Epoch 475: 0.9426857142857142\n",
            "Epoch 476: 0.9429285714285714\n",
            "Epoch 477: 0.9430142857142857\n",
            "Epoch 478: 0.9431142857142857\n",
            "Epoch 479: 0.9429285714285714\n",
            "Epoch 480: 0.9427714285714286\n",
            "Epoch 481: 0.9427857142857143\n",
            "Epoch 482: 0.9429\n",
            "Epoch 483: 0.9431428571428572\n",
            "Epoch 484: 0.9433\n",
            "Epoch 485: 0.9433714285714285\n",
            "Epoch 486: 0.9434714285714285\n",
            "Epoch 487: 0.9435714285714286\n",
            "Epoch 488: 0.9435857142857142\n",
            "Epoch 489: 0.9435428571428571\n",
            "Epoch 490: 0.9435714285714286\n",
            "Epoch 491: 0.9435285714285714\n",
            "Epoch 492: 0.944\n",
            "Epoch 493: 0.9444142857142858\n",
            "Epoch 494: 0.9447428571428571\n",
            "Epoch 495: 0.9453\n",
            "Epoch 496: 0.9458285714285715\n",
            "Epoch 497: 0.9462714285714285\n",
            "Epoch 498: 0.9462\n",
            "Epoch 499: 0.9461571428571428\n",
            "Epoch 500: 0.9461285714285714\n",
            "Epoch 501: 0.9458857142857143\n",
            "Epoch 502: 0.9454857142857143\n",
            "Epoch 503: 0.9454428571428571\n",
            "Epoch 504: 0.9452428571428572\n",
            "Epoch 505: 0.9454714285714285\n",
            "Epoch 506: 0.9453142857142857\n",
            "Epoch 507: 0.9453285714285714\n",
            "Epoch 508: 0.9452571428571429\n",
            "Epoch 509: 0.9451285714285714\n",
            "Epoch 510: 0.9450571428571428\n",
            "Epoch 511: 0.9450428571428572\n",
            "Epoch 512: 0.9450142857142857\n",
            "Epoch 513: 0.9449142857142857\n",
            "Epoch 514: 0.9449142857142857\n",
            "Epoch 515: 0.9453\n",
            "Epoch 516: 0.9455142857142858\n",
            "Epoch 517: 0.9458857142857143\n",
            "Epoch 518: 0.946\n",
            "Epoch 519: 0.9460857142857143\n",
            "Epoch 520: 0.9461\n",
            "Epoch 521: 0.946\n",
            "Epoch 522: 0.9460857142857143\n",
            "Epoch 523: 0.9457857142857143\n",
            "Epoch 524: 0.9457571428571429\n",
            "Epoch 525: 0.9456428571428571\n",
            "Epoch 526: 0.9455571428571429\n",
            "Epoch 527: 0.9453285714285714\n",
            "Epoch 528: 0.9451428571428572\n",
            "Epoch 529: 0.9449285714285715\n",
            "Epoch 530: 0.9447714285714286\n",
            "Epoch 531: 0.9447\n",
            "Epoch 532: 0.9444428571428571\n",
            "Epoch 533: 0.9442571428571429\n",
            "Epoch 534: 0.9439142857142857\n",
            "Epoch 535: 0.9436285714285715\n",
            "Epoch 536: 0.9431\n",
            "Epoch 537: 0.9427857142857143\n",
            "Epoch 538: 0.9427571428571428\n",
            "Epoch 539: 0.9426714285714286\n",
            "Epoch 540: 0.9426428571428571\n",
            "Epoch 541: 0.9426714285714286\n",
            "Epoch 542: 0.9428\n",
            "Epoch 543: 0.9426857142857142\n",
            "Epoch 544: 0.9426571428571429\n",
            "Epoch 545: 0.9426571428571429\n",
            "Epoch 546: 0.9427571428571428\n",
            "Epoch 547: 0.9428857142857143\n",
            "Epoch 548: 0.9429\n",
            "Epoch 549: 0.9428285714285715\n",
            "Epoch 550: 0.9427142857142857\n",
            "Epoch 551: 0.9426\n",
            "Epoch 552: 0.9426428571428571\n",
            "Epoch 553: 0.9426428571428571\n",
            "Epoch 554: 0.9425571428571429\n",
            "Epoch 555: 0.9428\n",
            "Epoch 556: 0.9430714285714286\n",
            "Epoch 557: 0.9431571428571428\n",
            "Epoch 558: 0.9431\n",
            "Epoch 559: 0.9429285714285714\n",
            "Epoch 560: 0.9427285714285715\n",
            "Epoch 561: 0.9425\n",
            "Epoch 562: 0.9424714285714286\n",
            "Epoch 563: 0.9426142857142857\n",
            "Epoch 564: 0.9425285714285714\n",
            "Epoch 565: 0.9422714285714285\n",
            "Epoch 566: 0.9420571428571428\n",
            "Epoch 567: 0.9418857142857143\n",
            "Epoch 568: 0.9416857142857142\n",
            "Epoch 569: 0.9416285714285715\n",
            "Epoch 570: 0.9415714285714286\n",
            "Epoch 571: 0.9413857142857143\n",
            "Epoch 572: 0.9413571428571429\n",
            "Epoch 573: 0.9413714285714285\n",
            "Epoch 574: 0.9413714285714285\n",
            "Epoch 575: 0.9413285714285714\n",
            "Epoch 576: 0.9417428571428571\n",
            "Epoch 577: 0.9422\n",
            "Epoch 578: 0.9424428571428571\n",
            "Epoch 579: 0.9423714285714285\n",
            "Epoch 580: 0.9425285714285714\n",
            "Epoch 581: 0.9427\n",
            "Epoch 582: 0.9426142857142857\n",
            "Epoch 583: 0.9426714285714286\n",
            "Epoch 584: 0.9428\n",
            "Epoch 585: 0.9428571428571428\n",
            "Epoch 586: 0.9427857142857143\n",
            "Epoch 587: 0.9427142857142857\n",
            "Epoch 588: 0.9426571428571429\n",
            "Epoch 589: 0.9428285714285715\n",
            "Epoch 590: 0.9431428571428572\n",
            "Epoch 591: 0.9435\n",
            "Epoch 592: 0.9436\n",
            "Epoch 593: 0.9437571428571429\n",
            "Epoch 594: 0.9438285714285715\n",
            "Epoch 595: 0.9441142857142857\n",
            "Epoch 596: 0.9445857142857143\n",
            "Epoch 597: 0.9449285714285715\n",
            "Epoch 598: 0.9452285714285714\n",
            "Epoch 599: 0.9455428571428571\n",
            "Epoch 600: 0.9457\n",
            "Epoch 601: 0.9457285714285715\n",
            "Epoch 602: 0.9457142857142857\n",
            "Epoch 603: 0.9457428571428571\n",
            "Epoch 604: 0.9458428571428571\n",
            "Epoch 605: 0.9458\n",
            "Epoch 606: 0.9456857142857142\n",
            "Epoch 607: 0.9455285714285714\n",
            "Epoch 608: 0.9454142857142858\n",
            "Epoch 609: 0.9454428571428571\n",
            "Epoch 610: 0.9452857142857143\n",
            "Epoch 611: 0.9451142857142857\n",
            "Epoch 612: 0.9449\n",
            "Epoch 613: 0.9447714285714286\n",
            "Epoch 614: 0.9447285714285715\n",
            "Epoch 615: 0.9446285714285715\n",
            "Epoch 616: 0.9444857142857143\n",
            "Epoch 617: 0.9443857142857143\n",
            "Epoch 618: 0.9446285714285715\n",
            "Epoch 619: 0.9447428571428571\n",
            "Epoch 620: 0.9446571428571429\n",
            "Epoch 621: 0.9447285714285715\n",
            "Epoch 622: 0.9449285714285715\n",
            "Epoch 623: 0.9451428571428572\n",
            "Epoch 624: 0.9452142857142857\n",
            "Epoch 625: 0.9454142857142858\n",
            "Epoch 626: 0.9456\n",
            "Epoch 627: 0.9456428571428571\n",
            "Epoch 628: 0.9457857142857143\n",
            "Epoch 629: 0.9458571428571428\n",
            "Epoch 630: 0.9460142857142857\n",
            "Epoch 631: 0.9460285714285714\n",
            "Epoch 632: 0.9461285714285714\n",
            "Epoch 633: 0.9463\n",
            "Epoch 634: 0.9462714285714285\n",
            "Epoch 635: 0.9461714285714286\n",
            "Epoch 636: 0.9461285714285714\n",
            "Epoch 637: 0.9459285714285715\n",
            "Epoch 638: 0.9457428571428571\n",
            "Epoch 639: 0.9456428571428571\n",
            "Epoch 640: 0.9453714285714285\n",
            "Epoch 641: 0.9452285714285714\n",
            "Epoch 642: 0.9450857142857143\n",
            "Epoch 643: 0.9449142857142857\n",
            "Epoch 644: 0.9449571428571428\n",
            "Epoch 645: 0.945\n",
            "Epoch 646: 0.9449714285714286\n",
            "Epoch 647: 0.9450714285714286\n",
            "Epoch 648: 0.9451285714285714\n",
            "Epoch 649: 0.9455428571428571\n",
            "Epoch 650: 0.9454571428571429\n",
            "Epoch 651: 0.9456\n",
            "Epoch 652: 0.9456285714285714\n",
            "Epoch 653: 0.9455\n",
            "Epoch 654: 0.9454142857142858\n",
            "Epoch 655: 0.9453428571428572\n",
            "Epoch 656: 0.9454285714285714\n",
            "Epoch 657: 0.9454571428571429\n",
            "Epoch 658: 0.9454142857142858\n",
            "Epoch 659: 0.9453857142857143\n",
            "Epoch 660: 0.9451571428571428\n",
            "Epoch 661: 0.9450571428571428\n",
            "Epoch 662: 0.9448\n",
            "Epoch 663: 0.9447714285714286\n",
            "Epoch 664: 0.9447285714285715\n",
            "Epoch 665: 0.9447\n",
            "Epoch 666: 0.9446142857142857\n",
            "Epoch 667: 0.9445142857142858\n",
            "Epoch 668: 0.9444571428571429\n",
            "Epoch 669: 0.9445142857142858\n",
            "Epoch 670: 0.9445428571428571\n",
            "Epoch 671: 0.9445428571428571\n",
            "Epoch 672: 0.9444571428571429\n",
            "Epoch 673: 0.9447714285714286\n",
            "Epoch 674: 0.9450142857142857\n",
            "Epoch 675: 0.9451857142857143\n",
            "Epoch 676: 0.9455857142857143\n",
            "Epoch 677: 0.9459428571428572\n",
            "Epoch 678: 0.9462285714285714\n",
            "Epoch 679: 0.9465142857142858\n",
            "Epoch 680: 0.9465428571428571\n",
            "Epoch 681: 0.9465857142857143\n",
            "Epoch 682: 0.9466\n",
            "Epoch 683: 0.9465428571428571\n",
            "Epoch 684: 0.9465857142857143\n",
            "Epoch 685: 0.9466285714285714\n",
            "Epoch 686: 0.9465142857142858\n",
            "Epoch 687: 0.9463571428571429\n",
            "Epoch 688: 0.9464\n",
            "Epoch 689: 0.9463142857142857\n",
            "Epoch 690: 0.9462285714285714\n",
            "Epoch 691: 0.9461142857142857\n",
            "Epoch 692: 0.9460142857142857\n",
            "Epoch 693: 0.9459857142857143\n",
            "Epoch 694: 0.9458142857142857\n",
            "Epoch 695: 0.9456714285714286\n",
            "Epoch 696: 0.9454285714285714\n",
            "Epoch 697: 0.9453571428571429\n",
            "Epoch 698: 0.9451714285714286\n",
            "Epoch 699: 0.9451\n",
            "Epoch 700: 0.9449\n",
            "Epoch 701: 0.9449142857142857\n",
            "Epoch 702: 0.9451285714285714\n",
            "Epoch 703: 0.9453285714285714\n",
            "Epoch 704: 0.9452\n",
            "Epoch 705: 0.9453571428571429\n",
            "Epoch 706: 0.9452285714285714\n",
            "Epoch 707: 0.9452857142857143\n",
            "Epoch 708: 0.9453857142857143\n",
            "Epoch 709: 0.9455857142857143\n",
            "Epoch 710: 0.9455428571428571\n",
            "Epoch 711: 0.9455857142857143\n",
            "Epoch 712: 0.9456285714285714\n",
            "Epoch 713: 0.9458428571428571\n",
            "Epoch 714: 0.9461285714285714\n",
            "Epoch 715: 0.9462857142857143\n",
            "Epoch 716: 0.9465142857142858\n",
            "Epoch 717: 0.9466285714285714\n",
            "Epoch 718: 0.9467142857142857\n",
            "Epoch 719: 0.9468142857142857\n",
            "Epoch 720: 0.9469285714285715\n",
            "Epoch 721: 0.9469857142857143\n",
            "Epoch 722: 0.947\n",
            "Epoch 723: 0.9470714285714286\n",
            "Epoch 724: 0.9470285714285714\n",
            "Epoch 725: 0.9469571428571428\n",
            "Epoch 726: 0.9469142857142857\n",
            "Epoch 727: 0.9468857142857143\n",
            "Epoch 728: 0.9469142857142857\n",
            "Epoch 729: 0.9465428571428571\n",
            "Epoch 730: 0.9465285714285714\n",
            "Epoch 731: 0.9464857142857143\n",
            "Epoch 732: 0.9463428571428572\n",
            "Epoch 733: 0.9464571428571429\n",
            "Epoch 734: 0.9465\n",
            "Epoch 735: 0.9465142857142858\n",
            "Epoch 736: 0.9464428571428571\n",
            "Epoch 737: 0.9463428571428572\n",
            "Epoch 738: 0.9462857142857143\n",
            "Epoch 739: 0.9462142857142857\n",
            "Epoch 740: 0.9461428571428572\n",
            "Epoch 741: 0.9461571428571428\n",
            "Epoch 742: 0.946\n",
            "Epoch 743: 0.9459285714285715\n",
            "Epoch 744: 0.9458\n",
            "Epoch 745: 0.9458285714285715\n",
            "Epoch 746: 0.9457571428571429\n",
            "Epoch 747: 0.9456714285714286\n",
            "Epoch 748: 0.9456714285714286\n",
            "Epoch 749: 0.9454428571428571\n",
            "Epoch 750: 0.9454285714285714\n",
            "Epoch 751: 0.9453857142857143\n",
            "Epoch 752: 0.9453714285714285\n",
            "Epoch 753: 0.9455142857142858\n",
            "Epoch 754: 0.9455142857142858\n",
            "Epoch 755: 0.9455714285714286\n",
            "Epoch 756: 0.9456285714285714\n",
            "Epoch 757: 0.9455571428571429\n",
            "Epoch 758: 0.9456\n",
            "Epoch 759: 0.9456\n",
            "Epoch 760: 0.9456285714285714\n",
            "Epoch 761: 0.9457428571428571\n",
            "Epoch 762: 0.9458\n",
            "Epoch 763: 0.9459\n",
            "Epoch 764: 0.9458\n",
            "Epoch 765: 0.9459\n",
            "Epoch 766: 0.9459\n",
            "Epoch 767: 0.9459\n",
            "Epoch 768: 0.9459142857142857\n",
            "Epoch 769: 0.9458571428571428\n",
            "Epoch 770: 0.9460428571428572\n",
            "Epoch 771: 0.9460571428571428\n",
            "Epoch 772: 0.9460714285714286\n",
            "Epoch 773: 0.9460857142857143\n",
            "Epoch 774: 0.9461142857142857\n",
            "Epoch 775: 0.9461\n",
            "Epoch 776: 0.9461142857142857\n",
            "Epoch 777: 0.9461714285714286\n",
            "Epoch 778: 0.9462571428571429\n",
            "Epoch 779: 0.9462857142857143\n",
            "Epoch 780: 0.9461428571428572\n",
            "Epoch 781: 0.9460571428571428\n",
            "Epoch 782: 0.9460285714285714\n",
            "Epoch 783: 0.9461571428571428\n",
            "Epoch 784: 0.9462\n",
            "Epoch 785: 0.9462142857142857\n",
            "Epoch 786: 0.9462142857142857\n",
            "Epoch 787: 0.9462714285714285\n",
            "Epoch 788: 0.9462428571428572\n",
            "Epoch 789: 0.9462428571428572\n",
            "Epoch 790: 0.9461857142857143\n",
            "Epoch 791: 0.9461714285714286\n",
            "Epoch 792: 0.9462857142857143\n",
            "Epoch 793: 0.9463142857142857\n",
            "Epoch 794: 0.9462\n",
            "Epoch 795: 0.9461428571428572\n",
            "Epoch 796: 0.9460857142857143\n",
            "Epoch 797: 0.9459714285714286\n",
            "Epoch 798: 0.9458428571428571\n",
            "Epoch 799: 0.9458428571428571\n",
            "Epoch 800: 0.9457285714285715\n",
            "Epoch 801: 0.9456714285714286\n",
            "Epoch 802: 0.9457571428571429\n",
            "Epoch 803: 0.9456857142857142\n",
            "Epoch 804: 0.9456714285714286\n",
            "Epoch 805: 0.9457428571428571\n",
            "Epoch 806: 0.9458\n",
            "Epoch 807: 0.9458571428571428\n",
            "Epoch 808: 0.9458714285714286\n",
            "Epoch 809: 0.9459\n",
            "Epoch 810: 0.9459857142857143\n",
            "Epoch 811: 0.9460285714285714\n",
            "Epoch 812: 0.9460285714285714\n",
            "Epoch 813: 0.9461\n",
            "Epoch 814: 0.9460571428571428\n",
            "Epoch 815: 0.9460857142857143\n",
            "Epoch 816: 0.946\n",
            "Epoch 817: 0.9459285714285715\n",
            "Epoch 818: 0.9458857142857143\n",
            "Epoch 819: 0.9457714285714286\n",
            "Epoch 820: 0.9456428571428571\n",
            "Epoch 821: 0.9456714285714286\n",
            "Epoch 822: 0.9456\n",
            "Epoch 823: 0.9455142857142858\n",
            "Epoch 824: 0.9455428571428571\n",
            "Epoch 825: 0.9455142857142858\n",
            "Epoch 826: 0.9453714285714285\n",
            "Epoch 827: 0.9453428571428572\n",
            "Epoch 828: 0.9453857142857143\n",
            "Epoch 829: 0.9453\n",
            "Epoch 830: 0.9453285714285714\n",
            "Epoch 831: 0.9452285714285714\n",
            "Epoch 832: 0.9451428571428572\n",
            "Epoch 833: 0.9451285714285714\n",
            "Epoch 834: 0.9450142857142857\n",
            "Epoch 835: 0.9449571428571428\n",
            "Epoch 836: 0.9449\n",
            "Epoch 837: 0.9448714285714286\n",
            "Epoch 838: 0.9447142857142857\n",
            "Epoch 839: 0.9446714285714286\n",
            "Epoch 840: 0.9446142857142857\n",
            "Epoch 841: 0.9445428571428571\n",
            "Epoch 842: 0.9445428571428571\n",
            "Epoch 843: 0.9446714285714286\n",
            "Epoch 844: 0.9448714285714286\n",
            "Epoch 845: 0.9450857142857143\n",
            "Epoch 846: 0.9451714285714286\n",
            "Epoch 847: 0.9453571428571429\n",
            "Epoch 848: 0.9454\n",
            "Epoch 849: 0.9454285714285714\n",
            "Epoch 850: 0.9456\n",
            "Epoch 851: 0.9457571428571429\n",
            "Epoch 852: 0.9459\n",
            "Epoch 853: 0.9458857142857143\n",
            "Epoch 854: 0.9459\n",
            "Epoch 855: 0.9460428571428572\n",
            "Epoch 856: 0.9461571428571428\n",
            "Epoch 857: 0.9462571428571429\n",
            "Epoch 858: 0.9463714285714285\n",
            "Epoch 859: 0.9464142857142858\n",
            "Epoch 860: 0.9465\n",
            "Epoch 861: 0.9466285714285714\n",
            "Epoch 862: 0.9467857142857142\n",
            "Epoch 863: 0.9467571428571429\n",
            "Epoch 864: 0.9468714285714286\n",
            "Epoch 865: 0.9470142857142857\n",
            "Epoch 866: 0.947\n",
            "Epoch 867: 0.947\n",
            "Epoch 868: 0.9468714285714286\n",
            "Epoch 869: 0.9469\n",
            "Epoch 870: 0.9469285714285715\n",
            "Epoch 871: 0.9468142857142857\n",
            "Epoch 872: 0.9467571428571429\n",
            "Epoch 873: 0.9466428571428571\n",
            "Epoch 874: 0.9465714285714286\n",
            "Epoch 875: 0.9465428571428571\n",
            "Epoch 876: 0.9465285714285714\n",
            "Epoch 877: 0.9464857142857143\n",
            "Epoch 878: 0.9463714285714285\n",
            "Epoch 879: 0.9463714285714285\n",
            "Epoch 880: 0.9462428571428572\n",
            "Epoch 881: 0.9461285714285714\n",
            "Epoch 882: 0.9461142857142857\n",
            "Epoch 883: 0.9461\n",
            "Epoch 884: 0.9461\n",
            "Epoch 885: 0.9461285714285714\n",
            "Epoch 886: 0.9461857142857143\n",
            "Epoch 887: 0.9462428571428572\n",
            "Epoch 888: 0.9462571428571429\n",
            "Epoch 889: 0.9461714285714286\n",
            "Epoch 890: 0.9462285714285714\n",
            "Epoch 891: 0.9463142857142857\n",
            "Epoch 892: 0.9463714285714285\n",
            "Epoch 893: 0.9463714285714285\n",
            "Epoch 894: 0.9464\n",
            "Epoch 895: 0.9463714285714285\n",
            "Epoch 896: 0.9464428571428571\n",
            "Epoch 897: 0.9464142857142858\n",
            "Epoch 898: 0.9463714285714285\n",
            "Epoch 899: 0.9463571428571429\n",
            "Epoch 900: 0.9463142857142857\n"
          ]
        }
      ],
      "source": [
        "for ep in range(Epoch+1):\n",
        "        loss_train, acc_train = epoch('train', federated_trainloader, net, optimizer, criterion)\n",
        "        if ep in lr_schedule:\n",
        "            lr *= 0.1\n",
        "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "        loss, acc = epoch('test', valloader, net, optimizer, criterion)\n",
        "        print(\"Epoch {}: {}\".format(ep, acc))"
      ],
      "id": "d7ae2880"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7e2422c",
        "outputId": "5b10606f-0e9b-4c07-92f0-0cb61203b987"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.18464524394497275, 0.9452285714285714)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "epoch('test', valloader, net, optimizer, criterion)"
      ],
      "id": "d7e2422c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd37ee5d"
      },
      "outputs": [],
      "source": [],
      "id": "cd37ee5d"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}